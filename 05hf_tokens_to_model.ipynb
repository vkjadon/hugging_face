{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv1ef957jzpTgnKzIXv7bW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjadon/llm/blob/main/05hf_tokens_to_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model"
      ],
      "metadata": {
        "id": "nCySCuYkOS5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "VlZOfnaUOXH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K1Edi39NxC_"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It also handles multiple sequences at a time, with no change in the API:"
      ],
      "metadata": {
        "id": "Q8k5gvTOOc7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequences)"
      ],
      "metadata": {
        "id": "n8nFpfKJOcZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "print(len(model_inputs[\"input_ids\"][0]))\n",
        "print(len(model_inputs[\"input_ids\"][1]))\n",
        "print(model_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "O7kHfZDoP36U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "print(len(model_inputs[\"input_ids\"][0]))\n",
        "print(model_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "eViBoTI0RZ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "print(len(model_inputs[\"input_ids\"][0]))\n",
        "print(len(model_inputs[\"input_ids\"][1]))\n",
        "print(model_inputs[\"input_ids\"][1])"
      ],
      "metadata": {
        "id": "CqxxiOEZRfFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "print(len(model_inputs[\"input_ids\"][0]))\n",
        "print(len(model_inputs[\"input_ids\"][1]))\n",
        "print(model_inputs[\"input_ids\"][1])"
      ],
      "metadata": {
        "id": "-S83pXRqP8pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
        "print(len(model_inputs[\"input_ids\"][0]))\n",
        "print(len(model_inputs[\"input_ids\"][1]))\n",
        "print(model_inputs[\"input_ids\"][1])"
      ],
      "metadata": {
        "id": "g6XCvQeyS1kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. For example, in the following code sample we are prompting the tokenizer to return tensors from the different frameworks — \"pt\" returns PyTorch tensors and \"np\" returns NumPy arrays"
      ],
      "metadata": {
        "id": "HiL7dxkOQI3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns PyTorch tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "print(len(model_inputs[\"input_ids\"][0]))\n",
        "print(len(model_inputs[\"input_ids\"][1]))\n",
        "print(type(model_inputs[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "qtLLw35rQIVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns NumPy arrays\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
        "print(len(model_inputs[\"input_ids\"][0]))\n",
        "print(len(model_inputs[\"input_ids\"][1]))\n",
        "print(type(model_inputs[\"input_ids\"][1]))"
      ],
      "metadata": {
        "id": "c1NQq2aZS95H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:"
      ],
      "metadata": {
        "id": "i8fBgdW4QeKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "oHr07rgIQe5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One token ID was added at the beginning, and one at the end. Let’s decode the two sequences of IDs above to see what this is about:"
      ],
      "metadata": {
        "id": "ilr3uRECQpbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "t8tEGoDGQmYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don’t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you."
      ],
      "metadata": {
        "id": "BzBiPr9QQ42C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "FX4bnOsePKWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "g-rOX8rzPQ54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**model_inputs)"
      ],
      "metadata": {
        "id": "yM-5dvOQPaIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)"
      ],
      "metadata": {
        "id": "LlSvUStiPm3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(output))"
      ],
      "metadata": {
        "id": "JSC6Zj2HTsZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(output))"
      ],
      "metadata": {
        "id": "zAW2VM8ATu_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model prediction scores."
      ],
      "metadata": {
        "id": "Sf28wATfUt0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.logits)\n"
      ],
      "metadata": {
        "id": "FUD5bl-LTzEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8nAgQ7nNUx1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.output_hidden_states = True\n",
        "output = model(**tokens)\n",
        "print(len(output.hidden_states))   # num layers\n",
        "print(output.hidden_states[-1])    # final layer\n"
      ],
      "metadata": {
        "id": "1vzNPUbwVE3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.keys())\n"
      ],
      "metadata": {
        "id": "0MUK4OxHVQyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.values())\n"
      ],
      "metadata": {
        "id": "6K8doUeXVUzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in output.items():\n",
        "    print(k, type(v))\n"
      ],
      "metadata": {
        "id": "2hZPQwoLVYTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.get(\"logits\"))\n",
        "print(output.get(\"loss\", \"No loss present\"))  # default value\n"
      ],
      "metadata": {
        "id": "yRWb_gBWVdEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\n",
        "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "tokens = tok(\"I love robotics!\", return_tensors=\"pt\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "output = model(**tokens, labels=torch.tensor([1]))\n",
        "print(output.loss)"
      ],
      "metadata": {
        "id": "wE44az2yWdfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2   # spam or not spam\n",
        ")\n",
        "\n",
        "text = \"Congratulations! You won a lottery.\"\n",
        "tokens = tok(text, return_tensors=\"pt\")\n",
        "\n",
        "label = torch.tensor([1])   # 1 = spam, 0 = not spam\n",
        "\n",
        "output = model(**tokens, labels=label)\n",
        "\n",
        "print(output.loss)\n",
        "print(output.logits)\n"
      ],
      "metadata": {
        "id": "Bjxhx-brXaRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    \"distilbert-base-cased-distilled-squad\"\n",
        ")\n",
        "\n",
        "context = \"The Eiffel Tower is located in Paris, France.\"\n",
        "question = \"Where is the Eiffel Tower located?\"\n",
        "\n",
        "tokens = tok(question, context, return_tensors=\"pt\")\n",
        "\n",
        "start_label = torch.tensor([6])\n",
        "end_label = torch.tensor([6])    # \"Paris\"\n",
        "\n",
        "output = model(\n",
        "    **tokens,\n",
        "    start_positions=start_label,\n",
        "    end_positions=end_label\n",
        ")\n",
        "\n",
        "print(output.loss)\n",
        "print(output.start_logits, output.end_logits)\n"
      ],
      "metadata": {
        "id": "aqhYI1sQXti-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "text = \"The Indian Constitution was adopted in 1949 and came into effect in 1950.\"\n",
        "\n",
        "tokens = tok(text, return_tensors=\"pt\")\n",
        "\n",
        "summary_ids = tok(\"Indian Constitution adopted in 1949.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "output = model(**tokens, labels=summary_ids)\n",
        "\n",
        "print(output.loss)"
      ],
      "metadata": {
        "id": "eNvTUcZGYc8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "\n",
        "text = \"Book a flight to Delhi\"\n",
        "\n",
        "# 0 = BookFlight, 1 = CancelTicket, 2 = WeatherQuery\n",
        "label = torch.tensor([0])\n",
        "\n",
        "tokens = tok(text, return_tensors=\"pt\")\n",
        "\n",
        "output = model(**tokens, labels=label)\n",
        "\n",
        "print(output.loss)\n",
        "print(output.logits)\n"
      ],
      "metadata": {
        "id": "UFL0YI-cYjaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"Congratulations! You won a lottery of $5000. Click here to claim.\",\n",
        "        \"Limited-time offer! Free coupons waiting for you.\",\n",
        "        \"Urgent: Your bank account will be closed. Verify now.\",\n",
        "        \"Hi John, can we meet tomorrow regarding the project?\",\n",
        "        \"Dear team, here is the report from last week.\",\n",
        "        \"Hello, your Amazon order has been shipped successfully.\",\n",
        "    ],\n",
        "    \"label\": [\n",
        "        1, 1, 1, 0, 0, 0\n",
        "    ]\n",
        "    # 1:spam examples 0:not-spam examples\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data)\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "wgLWqCNjZcbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "liC2isARZ1Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize, batched=True)\n",
        "tokenized_ds = tokenized_ds.train_test_split(test_size=0.2)\n"
      ],
      "metadata": {
        "id": "NPwtsKSCZ54P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"spam_model\",\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "s6RGmSb5aCBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "kk3mwfn8a236"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "def classify_mail(text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    output = model(**tokens)\n",
        "\n",
        "    probs = softmax(output.logits, dim=1)\n",
        "    pred = torch.argmax(probs).item()\n",
        "\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Spam Probability:\", probs[0][1].item())\n",
        "    print(\"Not-Spam Probability:\", probs[0][0].item())\n",
        "    print(\"Prediction:\", \"SPAM\" if pred == 1 else \"NOT SPAM\")\n",
        "\n",
        "# Test Email\n",
        "email = \"\"\"\n",
        "Dear user,\n",
        "Your account has been temporarily locked due to unusual activity.\n",
        "Click the link below to verify your identity.\n",
        "\"\"\"\n",
        "\n",
        "classify_mail(email)\n"
      ],
      "metadata": {
        "id": "zmGSS5s5bImU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "more_data = {\n",
        "    \"text\": [\n",
        "        \"Your OTP is 23456. Do not share.\",\n",
        "        \"Free vacation! Book now!\",\n",
        "        \"Meeting postponed to Monday.\",\n",
        "        \"Earn $500 per day working from home!\"\n",
        "    ],\n",
        "    \"label\": [0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "new_ds = Dataset.from_dict(more_data)\n",
        "dataset = Dataset.from_dict({\n",
        "    \"text\": dataset[\"text\"] + new_ds[\"text\"],\n",
        "    \"label\": dataset[\"label\"] + new_ds[\"label\"],\n",
        "})\n"
      ],
      "metadata": {
        "id": "cKHgsXfcbNCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then rerun tokenization + training."
      ],
      "metadata": {
        "id": "QYhrsquSblwN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b91ZHm78bmcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}